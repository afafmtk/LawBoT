#from pypdf import PdfReader
import re
import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.text_splitter import CharacterTextSplitter

from PyPDF2 import PdfReader
import fitz 
import pymupdf 

import os
import unicodedata
#import spacy
#from spacy.lang.fr.stop_words import STOP_WORDS
#nlp = spacy.load("fr_core_news_sm")
import logging
logger = logging.getLogger(__name__)


def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        with open(pdf, "rb") as f:
            pdf_reader = PdfReader(f)
            for page in pdf_reader.pages:
                text += page.extract_text() or ""
    return text

def clean_text(text):
    """
    Nettoie et normalise le texte extrait en supprimant les caract√®res ind√©sirables,
    en normalisant l'unicode et en structurant le texte de mani√®re lisible.
    """
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\n+', '\n', text)
    text = re.sub(r'[^\w\s.,;:!?()\[\]\'"-]', '', text)
    text = re.sub(r'\s+([.,;:!?])', r'\1', text)
    text = re.sub(r'([.,;:!?])\s+', r'\1 ', text)
    text = text.strip()
    text = re.sub(r'Page\s?\d+', '', text, flags=re.IGNORECASE)
    text = text.lower()
    return text


def identify_column_markers(text):
    markers = []
    space_sequences = re.finditer(r'\s{4,}', text)
    for match in space_sequences:
        markers.append(match.start())
    return markers


def reorganize_columns(text, markers):
    if not markers:
        return text
    columns = []
    start = 0
    for marker in markers:
        columns.append(text[start:marker].strip())
        start = marker
    columns.append(text[start:].strip())
    return '\n'.join(columns)


def analyze_page_structure(text):
    column_markers = identify_column_markers(text)
    structured_text = reorganize_columns(text, column_markers)
    return structured_text


def extract_text_simple(pdf_path):
    """
    Extrait le texte d'un PDF en mode simple.
    """
    # Utiliser PDFHandler pour centraliser l'extraction de texte
    raw_text = get_pdf_text([pdf_path])  # PDFHandler attend une liste de fichiers
    cleaned_text = clean_text(raw_text)
    return cleaned_text


def extract_f_double(pdf_path):
    """
    Extrait et structure le texte pour un PDF √† double format.
    """
    # Utiliser PDFHandler pour extraire le texte
    raw_text = get_pdf_text([pdf_path])
    all_pages_text = []

    for text in raw_text.split("\n\n"):  # Supposer que les pages sont s√©par√©es par "\n\n"
        structured_text = analyze_page_structure(clean_text(text))
        all_pages_text.append(structured_text)

    return '\n'.join(all_pages_text)


def detect_pdf_format(pdf_path):
    try:
        # Ouvrir le PDF
        doc = pymupdf.open(pdf_path)
        page = doc.load_page(0)  # Analyse uniquement la premi√®re page

        # Extraction du texte avec la m√©thode des blocs
        blocs = page.get_text("dict")["blocks"]

        # Analyse des positions des blocs
        left_count = 0
        right_count = 0
        middle_x = page.rect.width / 2  # Trouve le milieu de la page

        for block in blocs:
            if "bbox" in block:  # V√©rifie si le bloc contient des coordonn√©es
                x0, _, x1, _ = block["bbox"]  # Coordonn√©es du bloc

                # Compte les blocs √† gauche et √† droite
                if x1 <= middle_x:  # Tout le bloc est √† gauche
                    left_count += 1
                elif x0 >= middle_x:  # Tout le bloc est √† droite
                    right_count += 1

        doc.close()  # Ferme le document apr√®s l'analyse

        # V√©rifie si les colonnes sont √©quilibr√©es
        if left_count > 0 and right_count > 0:
            return "double"  # Deux colonnes d√©tect√©es
        else:
            return "simple"  # Une seule colonne d√©tect√©e

    except Exception as e:
        return f"Erreur lors de la d√©tection : {e}"
    

def get_text_chunks(text):
    text_splitter = CharacterTextSplitter(
        separator=" ",
        chunk_size=500,
        chunk_overlap=50,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    return chunks


def extract_text_chunks_from_pdf(file_path):
    """
    Traite un fichier PDF, extrait le texte, g√©n√®re des chunks,
    et cr√©e un magasin de vecteurs en int√©grant le r√©sum√©.
    """
    format_type = detect_pdf_format(file_path)
    logger.info(f"üìù Format detected: {format_type}")

    if format_type == "empty":
        raise ValueError("Le PDF est vide ou ne contient aucune page.")

    if format_type == "double":
        text = extract_f_double(file_path)
    else:
        text = extract_text_simple(file_path)

    if not text.strip():
        raise ValueError("Aucun texte n'a pu √™tre extrait du PDF. Veuillez v√©rifier le fichier.")
    
    # summarized_text = summarize_text(text)
    text_chunks = get_text_chunks(text)
    if not text_chunks:
        raise ValueError("Le d√©coupage du texte a √©chou√© : aucun chunk n'a √©t√© g√©n√©r√©.")
    
    logger.info(f"{len(text_chunks)} chunks g√©n√©r√©s √† partir du texte r√©sum√©.")

    return text_chunks

    




